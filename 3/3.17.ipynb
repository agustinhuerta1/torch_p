{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding Layers in RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "\n",
    "#Recurrent, Embedding & Dropout Layers\n",
    "inputs = [[1, 2, 3], [1, 0, 4], [1, 2, 4], [1, 4, 0], [1, 3, 3]]\n",
    "x = Variable(torch.LongTensor(inputs))\n",
    "embedding = nn.Embedding(num_embeddings=5, embedding_dim=20, padding_idx=1)\n",
    "drop = nn.Dropout(p=0.5)\n",
    "gru = nn.GRU(input_size=20, hidden_size=50, num_layers=2, batch_first=True,bidirectional=True, dropout=0.3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding size :  torch.Size([5, 3, 20])\n",
      "GRU hidden states size :  torch.Size([5, 3, 100])\n",
      "GRU last hidden state size :  torch.Size([4, 5, 50])\n"
     ]
    }
   ],
   "source": [
    "emb = drop(embedding(x))\n",
    "gru_h, gru_h_t = gru(emb)\n",
    "\n",
    "print('Embedding size : ', emb.size())\n",
    "print('GRU hidden states size : ', gru_h.size())\n",
    "print('GRU last hidden state size : ', gru_h_t.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "         [-0.0000, -0.0000, -0.0000, -0.1707,  2.6281, -2.1161, -0.0000,\n",
       "           0.0000, -1.9245,  0.0000, -0.0000, -0.0000,  0.0000,  0.0000,\n",
       "           0.8809, -4.3065,  0.0000, -0.3075,  0.0000, -0.0000],\n",
       "         [ 0.0000, -0.4744, -0.0000,  0.0000,  0.9426, -0.4399,  0.6423,\n",
       "           0.0000,  0.0000,  0.0000,  0.0000,  2.0492, -0.5493,  1.3244,\n",
       "          -0.0000, -0.1863,  0.0000,  0.0000, -0.0000, -0.8903]],\n",
       "\n",
       "        [[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "         [-0.0000,  2.7458,  0.0000, -0.0000, -0.5961, -0.0000, -3.1967,\n",
       "           0.0000, -0.0000, -2.3899,  0.0000,  0.0000, -0.0000, -0.7566,\n",
       "           0.0000,  0.5724,  1.3032, -0.0000, -0.0000, -4.9827],\n",
       "         [ 0.0000,  1.4527, -0.0000,  0.0000,  0.0000, -0.0000,  0.0731,\n",
       "          -0.0000, -0.7196,  0.0000, -0.0000,  0.0000,  0.5107,  0.0000,\n",
       "           0.0000, -0.0000,  2.3160, -1.2674, -1.6862,  0.0000]],\n",
       "\n",
       "        [[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "         [-0.0000, -0.7969, -0.0000, -0.1707,  2.6281, -2.1161, -3.9292,\n",
       "           1.5646, -0.0000,  2.7830, -2.5453, -0.9409,  0.0000,  1.8264,\n",
       "           0.0000, -0.0000,  0.0000, -0.3075,  3.1587, -0.0000],\n",
       "         [ 2.9154,  0.0000, -0.7981,  0.3827,  0.0000, -0.0000,  0.0000,\n",
       "          -1.9537, -0.7196,  0.0000, -0.0000,  1.1838,  0.5107,  0.0000,\n",
       "           0.0000, -2.9080,  2.3160, -1.2674, -1.6862,  2.6397]],\n",
       "\n",
       "        [[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  1.4527, -0.7981,  0.3827,  0.0000, -0.3232,  0.0000,\n",
       "          -0.0000, -0.0000,  0.0000, -0.0000,  0.0000,  0.0000,  0.8514,\n",
       "           0.0000, -0.0000,  0.0000, -0.0000, -1.6862,  0.0000],\n",
       "         [-0.0000,  2.7458,  3.1052, -0.9392, -0.5961, -0.0000, -0.0000,\n",
       "           0.0000, -1.1079, -2.3899,  2.0788,  1.7339, -0.7126, -0.0000,\n",
       "           0.0000,  0.0000,  1.3032, -4.7002, -0.0000, -4.9827]],\n",
       "\n",
       "        [[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.3701, -0.4744, -0.0000,  0.0000,  0.9426, -0.0000,  0.6423,\n",
       "           1.3987,  3.5380,  0.0000,  1.8697,  2.0492, -0.5493,  0.0000,\n",
       "          -0.0000, -0.1863,  0.0000,  0.0000, -0.0000, -0.8903],\n",
       "         [ 0.3701, -0.4744, -3.6415,  0.0000,  0.9426, -0.4399,  0.6423,\n",
       "           1.3987,  3.5380,  0.0000,  0.0000,  2.0492, -0.0000,  1.3244,\n",
       "          -0.0000, -0.1863,  0.0588,  0.0000, -0.0000, -0.0000]]],\n",
       "       grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\aaihs\\anaconda3\\envs\\pt\\lib\\site-packages (2.2.3)\n",
      "Requirement already satisfied: numpy>=1.22.4 in c:\\users\\aaihs\\anaconda3\\envs\\pt\\lib\\site-packages (from pandas) (1.26.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\aaihs\\anaconda3\\envs\\pt\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\aaihs\\anaconda3\\envs\\pt\\lib\\site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\aaihs\\anaconda3\\envs\\pt\\lib\\site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\aaihs\\anaconda3\\envs\\pt\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\aaihs\\anaconda3\\envs\\pt\\lib\\site-packages (4.67.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\aaihs\\anaconda3\\envs\\pt\\lib\\site-packages (from tqdm) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas\n",
    "!pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter, defaultdict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.nn import Embedding, Linear, LSTM, Module\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, SubsetRandomSampler\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class CharacterDataset(Dataset):\n",
    "    \"\"\"Custom dataset.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    text : str\n",
    "        Input text that will be used to create the entire database.\n",
    "\n",
    "    window_size : int\n",
    "        Number of characters to use as input features.\n",
    "\n",
    "    vocab_size : int\n",
    "        Number of characters in the vocabulary. Note that the last character\n",
    "        is always reserved for a special \"~\" out-of-vocabulary character.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    ch2ix : defaultdict\n",
    "        Mapping from the character to the position of that character in the\n",
    "        vocabulary. Note that all characters that are not in the vocabulary\n",
    "        will get mapped into the index `vocab_size - 1`.\n",
    "\n",
    "    ix2ch : dict\n",
    "        Mapping from the character position in the vocabulary to the actual\n",
    "        character.\n",
    "\n",
    "    vocabulary : list\n",
    "        List of all characters. `len(vocabulary) == vocab_size`.\n",
    "    \"\"\"\n",
    "    def __init__(self, text, window_size=1, vocab_size=50):\n",
    "        self.text = text.replace(\"\\n\", \" \")\n",
    "        self.window_size = window_size\n",
    "        self.ch2ix = defaultdict(lambda: vocab_size - 1)\n",
    "\n",
    "        most_common_ch2ix = {\n",
    "            x[0]: i\n",
    "            for i, x in enumerate(Counter(self.text).most_common()[: (vocab_size - 1)])\n",
    "        }\n",
    "        self.ch2ix.update(most_common_ch2ix)\n",
    "        self.ch2ix[\"~\"] = vocab_size - 1\n",
    "\n",
    "        self.ix2ch = {v: k for k, v in self.ch2ix.items()}\n",
    "        self.vocabulary = [self.ix2ch[i] for i in range(vocab_size)]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.text) - self.window_size\n",
    "\n",
    "    def __getitem__(self, ix):\n",
    "        X = torch.LongTensor(\n",
    "            [self.ch2ix[c] for c in self.text[ix : ix + self.window_size]]\n",
    "        )\n",
    "        y = self.ch2ix[self.text[ix + self.window_size]]\n",
    "\n",
    "        return X, y\n",
    "\n",
    "\n",
    "class Network(Module):\n",
    "    \"\"\"Custom network predicting the next character of a string.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    vocab_size : int\n",
    "        The number of characters in the vocabulary.\n",
    "\n",
    "    embedding_dim : int\n",
    "        Dimension of the character embedding vectors.\n",
    "\n",
    "    dense_dim : int\n",
    "        Number of neurons in the linear layer that follows the LSTM.\n",
    "\n",
    "    hidden_dim : int\n",
    "        Size of the LSTM hidden state.\n",
    "\n",
    "    max_norm : int\n",
    "        If any of the embedding vectors has a higher L2 norm than `max_norm`\n",
    "        it is rescaled.\n",
    "\n",
    "    n_layers : int\n",
    "        Number of the layers of the LSTM.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size,\n",
    "        embedding_dim=2,\n",
    "        dense_dim=32,\n",
    "        hidden_dim=8,\n",
    "        max_norm=2,\n",
    "        n_layers=1,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding = Embedding(\n",
    "                vocab_size,\n",
    "                embedding_dim,\n",
    "                padding_idx=vocab_size - 1,\n",
    "                norm_type=2,\n",
    "                max_norm=max_norm,\n",
    "        )\n",
    "        self.lstm = LSTM(\n",
    "                embedding_dim, hidden_dim, batch_first=True, num_layers=n_layers\n",
    "        )\n",
    "        self.linear_1 = Linear(hidden_dim, dense_dim)\n",
    "        self.linear_2 = Linear(dense_dim, vocab_size)\n",
    "\n",
    "\n",
    "    def forward(self, x, h=None, c=None):\n",
    "        \"\"\"Run the forward pass.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : torch.Tensor\n",
    "            Input tensor of shape `(n_samples, window_size)` of dtype\n",
    "            `torch.int64`.\n",
    "\n",
    "        h, c : torch.Tensor or None\n",
    "            Hidden states of the LSTM.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        logits : torch.Tensor\n",
    "            Tensor of shape `(n_samples, vocab_size)`.\n",
    "\n",
    "        h, c : torch.Tensor or None\n",
    "            Hidden states of the LSTM.\n",
    "        \"\"\"\n",
    "        emb = self.embedding(x)  # (n_samples, window_size, embedding_dim)\n",
    "        if h is not None and c is not None:\n",
    "            _, (h, c) = self.lstm(emb, (h, c))\n",
    "        else:\n",
    "            _, (h, c) = self.lstm(emb)  # (n_layers, n_samples, hidden_dim)\n",
    "\n",
    "        h_mean = h.mean(dim=0)  # (n_samples, hidden_dim)\n",
    "        x = self.linear_1(h_mean)  # (n_samples, dense_dim)\n",
    "        logits = self.linear_2(x)  # (n_samples, vocab_size)\n",
    "\n",
    "        return logits, h, c\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def compute_loss(cal, net, dataloader):\n",
    "    \"\"\"Computer average loss over a dataset.\"\"\"\n",
    "    net.eval()\n",
    "    all_losses = []\n",
    "    for X_batch, y_batch in dataloader:\n",
    "        probs, _, _ = net(X_batch)\n",
    "\n",
    "        all_losses.append(cal(probs, y_batch).item())\n",
    "\n",
    "    return np.mean(all_losses)\n",
    "\n",
    "def generate_text(n_chars, net, dataset, initial_text=\"Hello\", random_state=None):\n",
    "    \"\"\"Generate text with the character-level model.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_chars : int\n",
    "        Number of characters to generate.\n",
    "\n",
    "    net : Module\n",
    "        Character-level model.\n",
    "\n",
    "    dataset : CharacterDataset\n",
    "        Instance of the `CharacterDataset`.\n",
    "\n",
    "    initial_text : str\n",
    "        The starting text to be used as the initial condition for the model.\n",
    "\n",
    "    random_state : None or int\n",
    "        If not None, then the result is reproducible.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    res : str\n",
    "        Generated text.\n",
    "    \"\"\"\n",
    "    if not initial_text:\n",
    "        raise ValueError(\"You need to specify the initial text\")\n",
    "\n",
    "    res = initial_text\n",
    "    net.eval()\n",
    "    h, c = None, None\n",
    "\n",
    "    if random_state is not None:\n",
    "        np.random.seed(random_state)\n",
    "\n",
    "    for _ in range(n_chars):\n",
    "        previous_chars = initial_text if res == initial_text else res[-1]\n",
    "        features = torch.LongTensor([[dataset.ch2ix[c] for c in previous_chars]])\n",
    "        logits, h, c = net(features, h, c)\n",
    "        probs = F.softmax(logits[0], dim=0).detach().numpy()\n",
    "        new_ch = np.random.choice(dataset.vocabulary, p=probs)\n",
    "        res += new_ch\n",
    "\n",
    "    return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/6405 [00:04<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, train_loss=4.351, val_loss=4.351\n",
      "I hope it works Ecz7QH:EJPtbo1\"BgI v5f.UtoCF VRnqA€7 yP.,?Uyây\"LEn?SrsydU?cDSn2'dw”P~F”sUvcFrc!D7HiOmu;k7r2Bsh7wngJS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6405/6405 [01:30<00:00, 71.01it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, train_loss=1.792, val_loss=1.797\n",
      "I hope it works  ad.\"  Shat  gorse  I  be  peny  so,  wond.  I  cen  thero  it  to  Crook  paoud.  Wood  that  to  d\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6405/6405 [01:47<00:00, 59.67it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2, train_loss=1.759, val_loss=1.770\n",
      "I hope it works  arbbislofe  come  ovenp  dlerlerte,  wondper  is  stet  ut  the  dikint,  ge  dusen  hem  we  a  lh\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6405/6405 [02:05<00:00, 51.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3, train_loss=1.740, val_loss=1.753\n",
      "I hope it works  alfârtard  exad   nlerp  ruel,  of.  He  beoned  lone  bitht  on  wruin,  geade.  âr  exews  I  not\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6405/6405 [01:44<00:00, 61.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4, train_loss=1.736, val_loss=1.748\n",
      "I hope it works  thry  mowe  for   nwt   epfelf  hav  was  the  meod  tother  for  ubkook  cangh.  Wo  hroy  tre  ro\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6405/6405 [01:33<00:00, 68.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5, train_loss=1.739, val_loss=1.750\n",
      "I hope it works  thip  mice  you   nfe    sl  cest   as  with  is  see  but   gas  wliln.  Bethey  me  ap  u  I  ase\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6405/6405 [01:44<00:00, 61.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6, train_loss=1.726, val_loss=1.736\n",
      "I hope it works  alcOnallies  of   his   ecd  mestec  wond,  and  woonet,  so  he  wlid  osc  in,  me  ap  leeve  lo\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6405/6405 [01:40<00:00, 63.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7, train_loss=1.749, val_loss=1.760\n",
      "I hope it works  aly.  White  of   his    th  .tird'  word.  I  fert  told  it  to  Droagt   as  her  heg  we  tas  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6405/6405 [01:32<00:00, 68.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8, train_loss=1.718, val_loss=1.727\n",
      "I hope it works  alcOthurd  the  neab  be  perither\"  woouper  is  seet  u  it  to  Mun  a  took.  Wothecech  Wan  s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6405/6405 [01:45<00:00, 60.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9, train_loss=1.718, val_loss=1.730\n",
      "I hope it works  thipesmome  cowe  aye   euw  feore.  Matty  and  woon  bitht  on  wris  anle  spoom  odSell  ve  do\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6405/6405 [01:52<00:00, 57.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10, train_loss=1.727, val_loss=1.738\n",
      "I hope it works  adk,  mice  court  be    is  ferse,  wond,  and  when  binde  he  wlid  nale  ig  we  his  to,  as \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6405/6405 [01:47<00:00, 59.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 11, train_loss=1.718, val_loss=1.734\n",
      "I hope it works  alm-  I   neptnen  kaic   bed,  hav  wost,  and  whean  waid  nettivlon,  be  ip  me  oveus  ve  un\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6405/6405 [01:25<00:00, 75.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 12, train_loss=1.721, val_loss=1.734\n",
      "I hope it works  alc'sed   tot  a  his     \"  Dadouk  was  and  mead  a  waid  on  wliled  can  koof  orvelle  to  r\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6405/6405 [01:24<00:00, 75.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 13, train_loss=1.725, val_loss=1.740\n",
      "I hope it works  alc.  Muce  court  .ance  cell  som  unhokes  sitst  at,  so  on  wlilty  yastapt  at  I  leeke  da\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6405/6405 [01:24<00:00, 75.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 14, train_loss=1.709, val_loss=1.721\n",
      "I hope it works  arpker,   a  has  ovenp  lrews  sog  was  and  meson  time  exanted:,  qitce  ig  mandep  we  tast \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6405/6405 [01:20<00:00, 80.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 15, train_loss=1.719, val_loss=1.736\n",
      "I hope it works  alp'erchuehelales  I  be  gell  so,  moal.  I  me  to  boous  he  wlire.  Bethago  to  I  letken  s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6405/6405 [01:22<00:00, 77.41it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 66\u001b[0m\n\u001b[0;32m     63\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     65\u001b[0m train_loss \u001b[38;5;241m=\u001b[39m compute_loss(loss_f, net, train_dataloader)\n\u001b[1;32m---> 66\u001b[0m val_loss \u001b[38;5;241m=\u001b[39m \u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss_f\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnet\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     67\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_loss\u001b[38;5;132;01m=:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval_loss\u001b[38;5;132;01m=:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     69\u001b[0m \u001b[38;5;66;03m# Generate one sentence\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[24], line 5\u001b[0m, in \u001b[0;36mcompute_loss\u001b[1;34m(cal, net, dataloader)\u001b[0m\n\u001b[0;32m      3\u001b[0m net\u001b[38;5;241m.\u001b[39meval()\n\u001b[0;32m      4\u001b[0m all_losses \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m X_batch, y_batch \u001b[38;5;129;01min\u001b[39;00m dataloader:\n\u001b[0;32m      6\u001b[0m     probs, _, _ \u001b[38;5;241m=\u001b[39m net(X_batch)\n\u001b[0;32m      8\u001b[0m     all_losses\u001b[38;5;241m.\u001b[39mappend(cal(probs, y_batch)\u001b[38;5;241m.\u001b[39mitem())\n",
      "File \u001b[1;32mc:\\Users\\aaihs\\anaconda3\\envs\\pt\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:701\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    698\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    699\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    700\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 701\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    702\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    703\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    704\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[0;32m    705\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    706\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[0;32m    707\u001b[0m ):\n",
      "File \u001b[1;32mc:\\Users\\aaihs\\anaconda3\\envs\\pt\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:757\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    755\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    756\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 757\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    758\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    759\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\aaihs\\anaconda3\\envs\\pt\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\aaihs\\anaconda3\\envs\\pt\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[1;32mIn[23], line 49\u001b[0m, in \u001b[0;36mCharacterDataset.__getitem__\u001b[1;34m(self, ix)\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, ix):\n\u001b[1;32m---> 49\u001b[0m     X \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLongTensor\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     50\u001b[0m \u001b[43m        \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mch2ix\u001b[49m\u001b[43m[\u001b[49m\u001b[43mc\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mc\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m[\u001b[49m\u001b[43mix\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mix\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwindow_size\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m     51\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     52\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mch2ix[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtext[ix \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwindow_size]]\n\u001b[0;32m     54\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m X, y\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "if __name__ == \"__main__\":\n",
    "    with open(\"text.txt\", \"r\") as f:\n",
    "        text = \"\\n\".join(f.readlines())\n",
    "\n",
    "    # Hyperparameters model\n",
    "    vocab_size = 70\n",
    "    window_size = 10\n",
    "    embedding_dim = 2\n",
    "    hidden_dim = 16\n",
    "    dense_dim = 32\n",
    "    n_layers = 1\n",
    "    max_norm = 2\n",
    "\n",
    "    # Training config\n",
    "    n_epochs = 25\n",
    "    train_val_split = 0.8\n",
    "    batch_size = 128\n",
    "    random_state = 13\n",
    "\n",
    "    torch.manual_seed(random_state)\n",
    "\n",
    "    loss_f = torch.nn.CrossEntropyLoss()\n",
    "    dataset = CharacterDataset(text, window_size=window_size, vocab_size=vocab_size)\n",
    "\n",
    "    n_samples = len(dataset)\n",
    "    split_ix = int(n_samples * train_val_split)\n",
    "\n",
    "    train_indices, val_indices = np.arange(split_ix), np.arange(split_ix, n_samples)\n",
    "\n",
    "    train_dataloader = DataLoader(\n",
    "            dataset, sampler=SubsetRandomSampler(train_indices), batch_size=batch_size\n",
    "    )\n",
    "    val_dataloader = DataLoader(\n",
    "            dataset, sampler=SubsetRandomSampler(val_indices), batch_size=batch_size\n",
    "    )\n",
    "\n",
    "    net = Network(\n",
    "            vocab_size,\n",
    "            hidden_dim=hidden_dim,\n",
    "            n_layers=n_layers,\n",
    "            dense_dim=dense_dim,\n",
    "            embedding_dim=embedding_dim,\n",
    "            max_norm=max_norm,\n",
    "    )\n",
    "    optimizer = torch.optim.Adam(\n",
    "            net.parameters(),\n",
    "            lr=1e-2,\n",
    "    )\n",
    "\n",
    "    emb_history = []\n",
    "\n",
    "    for e in range(n_epochs + 1):\n",
    "        net.train()\n",
    "        for X_batch, y_batch in tqdm(train_dataloader):\n",
    "            if e == 0:\n",
    "                break\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            probs, _, _ = net(X_batch)\n",
    "            loss = loss_f(probs, y_batch)\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "        train_loss = compute_loss(loss_f, net, train_dataloader)\n",
    "        val_loss = compute_loss(loss_f, net, val_dataloader)\n",
    "        print(f\"Epoch: {e}, {train_loss=:.3f}, {val_loss=:.3f}\")\n",
    "\n",
    "        # Generate one sentence\n",
    "        initial_text = \"I hope it works \"\n",
    "        generated_text = generate_text(\n",
    "            100, net, dataset, initial_text=initial_text, random_state=random_state\n",
    "        )\n",
    "        print(generated_text)\n",
    "\n",
    "        # Prepare DataFrame\n",
    "        weights = net.embedding.weight.detach().clone().numpy()\n",
    "\n",
    "        df = pd.DataFrame(weights, columns=[f\"dim_{i}\" for i in range(embedding_dim)])\n",
    "        df[\"epoch\"] = e\n",
    "        df[\"character\"] = dataset.vocabulary\n",
    "\n",
    "        emb_history.append(df)\n",
    "\n",
    "final_df = pd.concat(emb_history)\n",
    "final_df.to_csv(\"res.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
