{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Processing Using PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Embedding\n",
    "\n",
    "predictive model for trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'data': 0, 'science': 1}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "torch.manual_seed(1234)\n",
    "word_to_ix = {\"data\": 0, \"science\": 1}\n",
    "word_to_ix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(2, 5)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeds = nn.Embedding(2, 5)  # 2 words in vocab, 5 dimensional embeddings\n",
    "embeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lookup_tensor = torch.tensor([word_to_ix[\"data\"]], dtype=torch.long)\n",
    "lookup_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0461,  0.4024, -1.0115,  0.2167, -0.6123]],\n",
       "       grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hello_embed = embeds(lookup_tensor)\n",
    "hello_embed "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(['The', 'popularity'], 'of'), (['popularity', 'of'], 'the'), (['of', 'the'], 'term')]\n"
     ]
    }
   ],
   "source": [
    "test_sentence = \"\"\"The popularity of the term \"data science\" has exploded \n",
    "in business environments and academia, as indicated by a jump in job \n",
    "openings.[32] However, many critical academics and journalists see no \n",
    "distinction between data science and statistics. Writing in Forbes, Gil \n",
    "Press argues that data science is a buzzword without a clear definition \n",
    "and has simply replaced \"business analytics\" in contexts such as graduate \n",
    "degree programs.[7] In the question-and-answer section of his keynote \n",
    "address at the Joint Statistical Meetings of American Statistical \n",
    "Association, noted applied statistician Nate Silver said, \"I think data\n",
    "scientist is a sexed up term for a statistician....Statistics is a branch \n",
    "of science. Data scientist is slightly redundant in some way and people \n",
    "shouldn't berate the term statistician.\"[9] Similarly, in business sector, \n",
    "multiple researchers and analysts state that data scientists alone are \n",
    "far from being sufficient in granting companies a real competitive \n",
    "advantage[33] and consider data scientists as only one of the four \n",
    "greater job families companies require to leverage big data effectively, \n",
    "namely: data analysts, data scientists, big data developers and big data \n",
    "engineers.[34]\n",
    " On the other hand, responses to criticism are as numerous. In a 2014 Wall \n",
    "Street Journal article, Irving Wladawsky-Berger compares the data science \n",
    "enthusiasm with the dawn of computer science. He argues data science, like \n",
    "any other interdisciplinary field, employs methodologies and practices from \n",
    "across the academia and industry, but then it will morph them into a new \n",
    "discipline. He brings to attention the sharp criticisms computer science, \n",
    "now a well respected academic discipline, had to once face.[35] Likewise, \n",
    "NYU Stern's Vasant Dhar, as do many other academic proponents of data \n",
    "science,[35] argues more specifically in December 2013 that data science is different from the \n",
    "existing practice of data analysis across all disciplines, which focuses \n",
    "only on explaining data sets. Data science seeks actionable and consistent \n",
    "pattern for predictive uses.[1] This practical engineering goal takes data \n",
    "science beyond traditional analytics. Now the data in those disciplines and \n",
    "applied fields that lacked solid theories, like health science and social \n",
    "science, could be sought and utilized to generate powerful predictive \n",
    "models.[1]\"\"\".split()\n",
    " # we should tokenize the input, but we will ignore that for now\n",
    " # build a list of tuples.  Each tuple is ([ word_i-2, word_i-1 ], target word)\n",
    "trigrams = [([test_sentence[i], test_sentence[i + 1]], test_sentence[i + 2]) for i in range(len(test_sentence) - 2)]\n",
    "# print the first 3, just so you can see what they look like\n",
    "print(trigrams[:3])\n",
    "vocab = set(test_sentence)\n",
    "word_to_ix = {word: i for i, word in enumerate(vocab)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NGramLanguageModeler(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, context_size):\n",
    "        super(NGramLanguageModeler, self).__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.linear1 = nn.Linear(context_size * embedding_dim, 128)\n",
    "        self.linear2 = nn.Linear(128, vocab_size)\n",
    "    def forward(self, inputs):\n",
    "        embeds = self.embeddings(inputs).view((1, -1))\n",
    "        out = F.relu(self.linear1(embeds))\n",
    "        out = self.linear2(out)\n",
    "        log_probs = F.log_softmax(out, dim=1)\n",
    "        return log_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONTEXT_SIZE = 2\n",
    "EMBEDDING_DIM = 10\n",
    "\n",
    "losses = []\n",
    "loss_function = nn.NLLLoss()\n",
    "model = NGramLanguageModeler(len(vocab), EMBEDDING_DIM, CONTEXT_SIZE)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1871.6984968185425, 1857.3277735710144, 1843.2285251617432, 1829.3953928947449, 1815.8193395137787, 1802.4981145858765, 1789.4459414482117, 1776.6738185882568, 1764.1895637512207, 1752.0007112026215, 1740.1305613517761, 1728.5821821689606, 1717.3550736904144, 1706.4400453567505, 1695.8412848711014, 1685.5456243753433, 1675.538502573967, 1665.7822011709213, 1656.2583233118057, 1646.9528653621674, 1637.8404148817062, 1628.8908122777939, 1620.0785230398178, 1611.3936021327972, 1602.8187881708145, 1594.3363060355186, 1585.9359219670296, 1577.6017825603485, 1569.3233283162117, 1561.083087027073, 1552.871501326561, 1544.6799331903458, 1536.4954044222832, 1528.312023639679, 1520.1358532905579, 1511.9567221403122, 1503.7672375440598, 1495.5567085146904, 1487.3234621882439, 1479.06174659729]\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(10):\n",
    "    total_loss = 0\n",
    "    for context, target in trigrams:\n",
    "        #  Step 1. Prepare the inputs to be passed to the model (i.e, turn the words\n",
    "        #  into integer indices and wrap them in tensors)\n",
    "        context_idxs = torch.tensor([word_to_ix[w] for w in context], dtype=torch.long)\n",
    "        #  Step 2. Recall that torch *accumulates* gradients. Before passing in \n",
    "         # new instance, you need to zero out the gradients from the old\n",
    "        # instance\n",
    "        model.zero_grad()\n",
    "        # Step 3. Run the forward pass, getting log probabilities over next\n",
    "        # words\n",
    "        log_probs = model(context_idxs)\n",
    "        #  Step 4. Compute your loss function. (Again, Torch wants the target\n",
    "        # word wrapped in a tensor)\n",
    "        loss = loss_function(log_probs, torch.tensor([word_to_ix[target]], dtype=torch.long))\n",
    "        # Step 5. Do the backward pass and update the gradient\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        #  Get the Python number from a 1-element Tensor by calling tensor.item()\n",
    "        total_loss += loss.item()\n",
    "    losses.append(total_loss)\n",
    "print(losses)  # The loss decreased every iteration over the training data!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CBOW Model in PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONTEXT_SIZE = 2  # 2 words to the left, 2 to the right \n",
    "raw_text = \"\"\"For \n",
    "the future of data science, Donoho projects an ever-growing environment for \n",
    "open science where data sets used for academic publications are accessible \n",
    "to all researchers.[36] US National Institute of Health has already \n",
    "announced plans to enhance reproducibility and transparency of research \n",
    "data.[39] Other big journals are likewise following suit.[40][41] This way, \n",
    "the future of data science not only exceeds the boundary of statistical \n",
    "theories in scale and methodology, but data science will revolutionize \n",
    "current academia and research paradigms.[36] As Donoho concludes, \"the \n",
    "scope and impact of data science will continue to expand enormously in \n",
    "coming decades as scientific data and data about science itself become \n",
    "ubiquitously available.\"[36]\"\"\".split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(['For', 'the', 'of', 'data'], 'future'), (['the', 'future', 'data', 'science,'], 'of'), (['future', 'of', 'science,', 'Donoho'], 'data'), (['of', 'data', 'Donoho', 'projects'], 'science,'), (['data', 'science,', 'projects', 'an'], 'Donoho')]\n"
     ]
    }
   ],
   "source": [
    "# By deriving a set from `raw_text`, we deduplicate the array\n",
    "vocab = set(raw_text)\n",
    "vocab_size = len(vocab)\n",
    "word_to_ix = {word: i for i, word in enumerate(vocab)}\n",
    "data = []\n",
    "for i in range(2, len(raw_text) - 2):\n",
    "    context = [raw_text[i - 2], raw_text[i - 1],\n",
    "    raw_text[i + 1], raw_text[i + 2]]\n",
    "    target = raw_text[i]\n",
    "    data.append((context, target))\n",
    "print(data[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class NGramLanguageModeler(nn.Module):\n",
    "#     def __init__(self, vocab_size, embedding_dim, context_size):\n",
    "#         super(NGramLanguageModeler, self).__init__()\n",
    "#         self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "#         self.linear1 = nn.Linear(context_size * embedding_dim, 128)\n",
    "#         self.linear2 = nn.Linear(128, vocab_size)\n",
    "#     def forward(self, inputs):\n",
    "#         embeds = self.embeddings(inputs).view((1, -1))\n",
    "#         out = F.relu(self.linear1(embeds))\n",
    "#         out = self.linear2(out)\n",
    "#         log_probs = F.log_softmax(out, dim=1)\n",
    "#         return log_probs\n",
    "class CBOW(nn.Module):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def forward(self, inputs):\n",
    "        pass\n",
    "    # create your model and train.  here are some functions to help you make (above code is an example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([25,  9, 72, 14])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the data ready for use by your module\n",
    "def make_context_vector(context, word_to_ix):\n",
    "    idxs = [word_to_ix[w] for w in context]\n",
    "    return torch.tensor(idxs, dtype=torch.long)\n",
    "make_context_vector(data[0][0], word_to_ix)  # example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.6877, -0.8275, -0.1261],\n",
      "        [ 0.3976, -0.4415,  0.6140]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "lin = nn.Linear(5, 3)  # maps from R^5 to R^3, parameters A, b\n",
    "# data is 2x5.  A maps from 5 to 3... can we map \"data\" under A?\n",
    "data = torch.randn(2, 5)\n",
    "print(lin(data))  # yes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0349, -0.6073],\n",
      "        [ 0.6068,  0.2397]])\n",
      "tensor([[0.0349, 0.0000],\n",
      "        [0.6068, 0.2397]])\n",
      "tensor([ 0.1256,  0.0864,  2.1068,  0.0520, -1.9631])\n",
      "tensor([0.0974, 0.0937, 0.7064, 0.0905, 0.0121])\n",
      "tensor(1.0000)\n",
      "tensor([-2.3288, -2.3681, -0.3476, -2.4025, -4.4175])\n"
     ]
    }
   ],
   "source": [
    "data = torch.randn(2, 2)\n",
    "print(data)\n",
    "print(F.relu(data))\n",
    "# Softmax is also in torch.nn.functional\n",
    "data = torch.randn(5)\n",
    "print(data)\n",
    "print(F.softmax(data, dim=0))\n",
    "print(F.softmax(data, dim=0).sum())  #  Sums to 1 because it is a distribution!\n",
    "print(F.log_softmax(data, dim=0))  # theres also log_softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm = nn.LSTM(3, 3)  # Input dim is 3, output dim is 3\n",
    "inputs = [torch.randn(1, 3) for _ in range(5)]  # make a sequence of length 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.0006, -0.2630,  0.1169]],\n",
      "\n",
      "        [[ 0.3497, -0.2128, -0.1850]],\n",
      "\n",
      "        [[ 0.0167, -0.1945, -0.0034]],\n",
      "\n",
      "        [[-0.0260,  0.0778,  0.0303]],\n",
      "\n",
      "        [[ 0.0114,  0.1984, -0.0474]]], grad_fn=<MkldnnRnnLayerBackward0>)\n",
      "(tensor([[[ 0.0114,  0.1984, -0.0474]]], grad_fn=<StackBackward0>), tensor([[[ 0.0191,  0.5876, -0.1018]]], grad_fn=<StackBackward0>))\n"
     ]
    }
   ],
   "source": [
    " # initialize the hidden state.\n",
    "hidden = (torch.randn(1, 1, 3),\n",
    "    torch.randn(1, 1, 3))\n",
    "for i in inputs:\n",
    "    # Step through the sequence one element at a time.\n",
    "    # after each step, hidden contains the hidden state.\n",
    "    out, hidden = lstm(i.view(1, 1, -1), hidden)\n",
    "inputs = torch.cat(inputs).view(len(inputs), 1, -1)\n",
    "hidden = (torch.randn(1, 1, 3), torch.randn(1, 1, 3))  #  clean out hidden state\n",
    "\n",
    "out, hidden = lstm(inputs, hidden)\n",
    "print(out)\n",
    "print(hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(['Probability',\n",
       "   'and',\n",
       "   'random',\n",
       "   'variable',\n",
       "   'are',\n",
       "   'integral',\n",
       "   'part',\n",
       "   'of',\n",
       "   'computation'],\n",
       "  ['DET', 'NN', 'V', 'DET', 'NN']),\n",
       " (['Understanding',\n",
       "   'of',\n",
       "   'the',\n",
       "   'probability',\n",
       "   'and',\n",
       "   'associated',\n",
       "   'concepts',\n",
       "   'are',\n",
       "   'essential'],\n",
       "  ['NN', 'V', 'DET', 'NN'])]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def prepare_sequence(seq, to_ix):\n",
    "    idxs = [to_ix[w] for w in seq]\n",
    "    return torch.tensor(idxs, dtype=torch.long)\n",
    "\n",
    "training_data = [\n",
    "    (\"Probability and random variable are integral part of computation \".split(),\n",
    "    [\"DET\", \"NN\", \"V\", \"DET\", \"NN\"]),\n",
    "    (\"Understanding of the probability and associated concepts are essential\".split(),\n",
    "    [\"NN\", \"V\", \"DET\", \"NN\"])\n",
    "]\n",
    "training_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Probability': 0, 'and': 1, 'random': 2, 'variable': 3, 'are': 4, 'integral': 5, 'part': 6, 'of': 7, 'computation': 8, 'Understanding': 9, 'the': 10, 'probability': 11, 'associated': 12, 'concepts': 13, 'essential': 14}\n"
     ]
    }
   ],
   "source": [
    "word_to_ix = {}\n",
    "for sent, tags in training_data:\n",
    "    for word in sent:\n",
    "        if word not in word_to_ix:\n",
    "            word_to_ix[word] = len(word_to_ix)\n",
    "print(word_to_ix)\n",
    "\n",
    "tag_to_ix = {\"DET\": 0, \"NN\": 1, \"V\": 2}\n",
    "\n",
    "EMBEDDING_DIM = 6\n",
    "HIDDEN_DIM = 6\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class LSTMTagger(nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_dim, vocab_size, tagset_size):\n",
    "        super(LSTMTagger, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        # The LSTM takes word embeddings as inputs, and outputs hidden states with dimensionality hidden_dim.\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim)\n",
    "        # The linear layer that maps from hidden state space to tag space\n",
    "        self.hidden2tag = nn.Linear(hidden_dim, tagset_size)\n",
    "        self.hidden = self.init_hidden()\n",
    "\n",
    "    def init_hidden(self):\n",
    "        # Before we've done anything, we don't have any hidden state.\n",
    "        # Refer to the Pytorch documentation to see exactly why they have this dimensionality.\n",
    "        # The axes semantics are (num_layers, minibatch_size, hidden_dim)\n",
    "        return (torch.zeros(1, 1, self.hidden_dim),\n",
    "                torch.zeros(1, 1, self.hidden_dim))\n",
    "\n",
    "    def forward(self, sentence):\n",
    "        embeds = self.word_embeddings(sentence)\n",
    "        lstm_out, self.hidden = self.lstm(embeds.view(len(sentence), 1, -1), self.hidden)\n",
    "        tag_space = self.hidden2tag(lstm_out.view(len(sentence), -1))\n",
    "        tag_scores = F.log_softmax(tag_space, dim=1)\n",
    "        return tag_scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTMTagger(\n",
      "  (word_embeddings): Embedding(15, 6)\n",
      "  (lstm): LSTM(6, 6)\n",
      "  (hidden2tag): Linear(in_features=6, out_features=3, bias=True)\n",
      ")\n",
      "NLLLoss()\n",
      "SGD (\n",
      "Parameter Group 0\n",
      "    dampening: 0\n",
      "    differentiable: False\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    lr: 0.1\n",
      "    maximize: False\n",
      "    momentum: 0\n",
      "    nesterov: False\n",
      "    weight_decay: 0\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = LSTMTagger(EMBEDDING_DIM, HIDDEN_DIM, len(word_to_ix), len(tag_to_ix))\n",
    "loss_function = nn.NLLLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "print(model)\n",
    "print(loss_function)\n",
    "print(optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.0341, -1.1599, -1.1058],\n",
      "        [-0.9666, -1.2035, -1.1410],\n",
      "        [-1.0629, -1.1646, -1.0715],\n",
      "        [-1.0081, -1.2106, -1.0875],\n",
      "        [-0.9322, -1.2693, -1.1230],\n",
      "        [-0.9678, -1.2411, -1.1056],\n",
      "        [-0.9196, -1.2446, -1.1607],\n",
      "        [-0.9279, -1.2243, -1.1691],\n",
      "        [-1.0630, -1.1596, -1.0759]])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    inputs = prepare_sequence(training_data[0][0], word_to_ix)\n",
    "    tag_scores = model(inputs)\n",
    "    print(tag_scores)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
