{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised Learning Using PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.optim\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.init as init\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x1</th>\n",
       "      <th>x2</th>\n",
       "      <th>x3_LCV</th>\n",
       "      <th>x3_QKP</th>\n",
       "      <th>x3_SAT</th>\n",
       "      <th>x3_XJB</th>\n",
       "      <th>x4_MZBER</th>\n",
       "      <th>x4_PQKE</th>\n",
       "      <th>x4_YEQA</th>\n",
       "      <th>x4_ZUQF</th>\n",
       "      <th>...</th>\n",
       "      <th>x7_5</th>\n",
       "      <th>x7_6</th>\n",
       "      <th>x7_8</th>\n",
       "      <th>x8_-7.5</th>\n",
       "      <th>x8_-6.5</th>\n",
       "      <th>x8_-5.5</th>\n",
       "      <th>x8_-4.5</th>\n",
       "      <th>x8_-3.5</th>\n",
       "      <th>x8_-2.5</th>\n",
       "      <th>x8_-1.5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-2.247572</td>\n",
       "      <td>-1.670521</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-2.247572</td>\n",
       "      <td>-1.670521</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.025939</td>\n",
       "      <td>1.391524</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-2.247572</td>\n",
       "      <td>-1.670521</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.692015</td>\n",
       "      <td>-0.035019</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1995</th>\n",
       "      <td>0.802121</td>\n",
       "      <td>0.880734</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996</th>\n",
       "      <td>0.227936</td>\n",
       "      <td>0.215753</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997</th>\n",
       "      <td>1.146373</td>\n",
       "      <td>0.183901</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1998</th>\n",
       "      <td>0.590802</td>\n",
       "      <td>0.479194</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999</th>\n",
       "      <td>0.785765</td>\n",
       "      <td>1.719597</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2000 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            x1        x2  x3_LCV  x3_QKP  x3_SAT  x3_XJB  x4_MZBER  x4_PQKE  \\\n",
       "0    -2.247572 -1.670521     0.0     0.0     1.0     0.0       1.0      0.0   \n",
       "1    -2.247572 -1.670521     0.0     0.0     1.0     0.0       1.0      0.0   \n",
       "2     1.025939  1.391524     1.0     0.0     0.0     0.0       1.0      0.0   \n",
       "3    -2.247572 -1.670521     0.0     0.0     1.0     0.0       1.0      0.0   \n",
       "4    -0.692015 -0.035019     0.0     0.0     0.0     1.0       1.0      0.0   \n",
       "...        ...       ...     ...     ...     ...     ...       ...      ...   \n",
       "1995  0.802121  0.880734     0.0     1.0     0.0     0.0       1.0      0.0   \n",
       "1996  0.227936  0.215753     0.0     0.0     0.0     1.0       0.0      0.0   \n",
       "1997  1.146373  0.183901     0.0     0.0     1.0     0.0       1.0      0.0   \n",
       "1998  0.590802  0.479194     1.0     0.0     0.0     0.0       1.0      0.0   \n",
       "1999  0.785765  1.719597     1.0     0.0     0.0     0.0       1.0      0.0   \n",
       "\n",
       "      x4_YEQA  x4_ZUQF  ...  x7_5  x7_6  x7_8  x8_-7.5  x8_-6.5  x8_-5.5  \\\n",
       "0         0.0      0.0  ...   0.0   1.0   0.0      0.0      1.0      0.0   \n",
       "1         0.0      0.0  ...   0.0   1.0   0.0      0.0      0.0      0.0   \n",
       "2         0.0      0.0  ...   0.0   0.0   0.0      0.0      0.0      0.0   \n",
       "3         0.0      0.0  ...   0.0   0.0   0.0      0.0      0.0      0.0   \n",
       "4         0.0      0.0  ...   1.0   0.0   0.0      0.0      0.0      0.0   \n",
       "...       ...      ...  ...   ...   ...   ...      ...      ...      ...   \n",
       "1995      0.0      0.0  ...   0.0   0.0   0.0      0.0      0.0      0.0   \n",
       "1996      1.0      0.0  ...   0.0   0.0   0.0      0.0      0.0      0.0   \n",
       "1997      0.0      0.0  ...   0.0   0.0   0.0      0.0      0.0      1.0   \n",
       "1998      0.0      0.0  ...   0.0   0.0   0.0      0.0      0.0      1.0   \n",
       "1999      0.0      0.0  ...   0.0   0.0   0.0      0.0      0.0      1.0   \n",
       "\n",
       "      x8_-4.5  x8_-3.5  x8_-2.5  x8_-1.5  \n",
       "0         0.0      0.0      0.0      0.0  \n",
       "1         1.0      0.0      0.0      0.0  \n",
       "2         0.0      0.0      1.0      0.0  \n",
       "3         1.0      0.0      0.0      0.0  \n",
       "4         1.0      0.0      0.0      0.0  \n",
       "...       ...      ...      ...      ...  \n",
       "1995      0.0      0.0      1.0      0.0  \n",
       "1996      0.0      0.0      1.0      0.0  \n",
       "1997      0.0      0.0      0.0      0.0  \n",
       "1998      0.0      0.0      0.0      0.0  \n",
       "1999      0.0      0.0      0.0      0.0  \n",
       "\n",
       "[2000 rows x 26 columns]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # open csv file and read data for pytorch\n",
    "import pandas as pd\n",
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "# import os\n",
    "# import time\n",
    "# import copy\n",
    "# import random\n",
    "# import math\n",
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.optim as optim\n",
    "# import torch.nn.functional as F\n",
    "# from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# read data\n",
    "data = pd.read_csv('dataset_Caso_1_processed.csv')\n",
    "data = data.dropna()\n",
    "X,Y = data.iloc[:,0:-1],data.iloc[:,-1]\n",
    "X\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x1</th>\n",
       "      <th>x2</th>\n",
       "      <th>x3_LCV</th>\n",
       "      <th>x3_QKP</th>\n",
       "      <th>x3_SAT</th>\n",
       "      <th>x3_XJB</th>\n",
       "      <th>x4_MZBER</th>\n",
       "      <th>x4_PQKE</th>\n",
       "      <th>x4_YEQA</th>\n",
       "      <th>x4_ZUQF</th>\n",
       "      <th>...</th>\n",
       "      <th>x7_5</th>\n",
       "      <th>x7_6</th>\n",
       "      <th>x7_8</th>\n",
       "      <th>x8_-7.5</th>\n",
       "      <th>x8_-6.5</th>\n",
       "      <th>x8_-5.5</th>\n",
       "      <th>x8_-4.5</th>\n",
       "      <th>x8_-3.5</th>\n",
       "      <th>x8_-2.5</th>\n",
       "      <th>x8_-1.5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-2.247571</td>\n",
       "      <td>-1.670513</td>\n",
       "      <td>1.336346e-05</td>\n",
       "      <td>-0.000007</td>\n",
       "      <td>9.999931e-01</td>\n",
       "      <td>3.070558e-07</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.000005</td>\n",
       "      <td>-1.462577e-06</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>...</td>\n",
       "      <td>-9.194131e-06</td>\n",
       "      <td>1.000006e+00</td>\n",
       "      <td>-5.642623e-06</td>\n",
       "      <td>0.000019</td>\n",
       "      <td>0.999997</td>\n",
       "      <td>-0.000017</td>\n",
       "      <td>0.000022</td>\n",
       "      <td>-0.000002</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>-1.600673e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-2.247578</td>\n",
       "      <td>-1.670526</td>\n",
       "      <td>2.848577e-05</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>9.999918e-01</td>\n",
       "      <td>1.084186e-05</td>\n",
       "      <td>1.000001</td>\n",
       "      <td>-0.000003</td>\n",
       "      <td>2.094801e-06</td>\n",
       "      <td>-0.000002</td>\n",
       "      <td>...</td>\n",
       "      <td>-8.121052e-06</td>\n",
       "      <td>9.999939e-01</td>\n",
       "      <td>8.609777e-06</td>\n",
       "      <td>-0.000003</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>-0.000011</td>\n",
       "      <td>1.000010</td>\n",
       "      <td>-0.000007</td>\n",
       "      <td>-0.000006</td>\n",
       "      <td>1.185335e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.025935</td>\n",
       "      <td>1.391526</td>\n",
       "      <td>1.000016e+00</td>\n",
       "      <td>-0.000010</td>\n",
       "      <td>1.201850e-05</td>\n",
       "      <td>1.271726e-06</td>\n",
       "      <td>1.000024</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>8.410662e-06</td>\n",
       "      <td>-0.000008</td>\n",
       "      <td>...</td>\n",
       "      <td>-5.184427e-09</td>\n",
       "      <td>1.879509e-05</td>\n",
       "      <td>-1.394652e-05</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>-0.000002</td>\n",
       "      <td>0.000023</td>\n",
       "      <td>0.999996</td>\n",
       "      <td>6.143002e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-2.247572</td>\n",
       "      <td>-1.670521</td>\n",
       "      <td>1.350241e-05</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>9.999891e-01</td>\n",
       "      <td>2.680728e-07</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>4.903438e-07</td>\n",
       "      <td>-0.000018</td>\n",
       "      <td>...</td>\n",
       "      <td>1.581487e-05</td>\n",
       "      <td>-1.290875e-06</td>\n",
       "      <td>9.696633e-07</td>\n",
       "      <td>-0.000014</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>1.000006</td>\n",
       "      <td>-0.000009</td>\n",
       "      <td>-0.000010</td>\n",
       "      <td>-2.666942e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.692005</td>\n",
       "      <td>-0.035029</td>\n",
       "      <td>2.371110e-06</td>\n",
       "      <td>0.000033</td>\n",
       "      <td>-2.908868e-06</td>\n",
       "      <td>9.999901e-01</td>\n",
       "      <td>0.999980</td>\n",
       "      <td>-0.000007</td>\n",
       "      <td>-2.803982e-06</td>\n",
       "      <td>0.000024</td>\n",
       "      <td>...</td>\n",
       "      <td>9.999890e-01</td>\n",
       "      <td>9.591986e-06</td>\n",
       "      <td>-3.521323e-06</td>\n",
       "      <td>-0.000005</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>-0.000006</td>\n",
       "      <td>1.000005</td>\n",
       "      <td>-0.000002</td>\n",
       "      <td>-0.000011</td>\n",
       "      <td>-1.149327e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1995</th>\n",
       "      <td>0.802149</td>\n",
       "      <td>0.880729</td>\n",
       "      <td>2.073757e-06</td>\n",
       "      <td>0.999996</td>\n",
       "      <td>2.357311e-07</td>\n",
       "      <td>-3.997955e-06</td>\n",
       "      <td>1.000014</td>\n",
       "      <td>-0.000004</td>\n",
       "      <td>3.130863e-06</td>\n",
       "      <td>-0.000012</td>\n",
       "      <td>...</td>\n",
       "      <td>-5.535543e-08</td>\n",
       "      <td>6.291697e-06</td>\n",
       "      <td>-1.015886e-05</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>-0.000013</td>\n",
       "      <td>-0.000011</td>\n",
       "      <td>-0.000007</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.781661e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996</th>\n",
       "      <td>0.227937</td>\n",
       "      <td>0.215761</td>\n",
       "      <td>4.018471e-07</td>\n",
       "      <td>-0.000009</td>\n",
       "      <td>-1.054975e-05</td>\n",
       "      <td>9.999933e-01</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>9.999761e-01</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>...</td>\n",
       "      <td>6.330760e-06</td>\n",
       "      <td>1.188480e-05</td>\n",
       "      <td>-1.891222e-05</td>\n",
       "      <td>-0.000009</td>\n",
       "      <td>-0.000011</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>-0.000021</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>1.000009</td>\n",
       "      <td>-9.735054e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997</th>\n",
       "      <td>1.146383</td>\n",
       "      <td>0.183903</td>\n",
       "      <td>2.062651e-05</td>\n",
       "      <td>-0.000007</td>\n",
       "      <td>1.000009e+00</td>\n",
       "      <td>8.974153e-06</td>\n",
       "      <td>1.000004</td>\n",
       "      <td>-0.000004</td>\n",
       "      <td>1.008618e-05</td>\n",
       "      <td>-0.000003</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.970408e-06</td>\n",
       "      <td>6.588313e-06</td>\n",
       "      <td>8.169397e-06</td>\n",
       "      <td>-0.000006</td>\n",
       "      <td>-0.000002</td>\n",
       "      <td>0.999986</td>\n",
       "      <td>0.000025</td>\n",
       "      <td>-0.000011</td>\n",
       "      <td>-0.000018</td>\n",
       "      <td>-8.635924e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1998</th>\n",
       "      <td>0.590797</td>\n",
       "      <td>0.479184</td>\n",
       "      <td>1.000018e+00</td>\n",
       "      <td>-0.000008</td>\n",
       "      <td>6.802510e-06</td>\n",
       "      <td>-1.569116e-06</td>\n",
       "      <td>1.000010</td>\n",
       "      <td>-0.000011</td>\n",
       "      <td>-1.523550e-05</td>\n",
       "      <td>-0.000010</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.392276e-06</td>\n",
       "      <td>-3.480053e-06</td>\n",
       "      <td>-1.184995e-05</td>\n",
       "      <td>-0.000018</td>\n",
       "      <td>-0.000010</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>-0.000009</td>\n",
       "      <td>-0.000001</td>\n",
       "      <td>-0.000007</td>\n",
       "      <td>-2.809606e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999</th>\n",
       "      <td>0.785764</td>\n",
       "      <td>1.719588</td>\n",
       "      <td>1.000007e+00</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>-1.485859e-05</td>\n",
       "      <td>6.585560e-06</td>\n",
       "      <td>0.999993</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>-9.329268e-06</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>...</td>\n",
       "      <td>1.192127e-05</td>\n",
       "      <td>6.428366e-07</td>\n",
       "      <td>8.511502e-06</td>\n",
       "      <td>-0.000027</td>\n",
       "      <td>-0.000001</td>\n",
       "      <td>0.999990</td>\n",
       "      <td>-0.000016</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>-1.323412e-05</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2000 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            x1        x2        x3_LCV    x3_QKP        x3_SAT        x3_XJB  \\\n",
       "0    -2.247571 -1.670513  1.336346e-05 -0.000007  9.999931e-01  3.070558e-07   \n",
       "1    -2.247578 -1.670526  2.848577e-05  0.000004  9.999918e-01  1.084186e-05   \n",
       "2     1.025935  1.391526  1.000016e+00 -0.000010  1.201850e-05  1.271726e-06   \n",
       "3    -2.247572 -1.670521  1.350241e-05  0.000013  9.999891e-01  2.680728e-07   \n",
       "4    -0.692005 -0.035029  2.371110e-06  0.000033 -2.908868e-06  9.999901e-01   \n",
       "...        ...       ...           ...       ...           ...           ...   \n",
       "1995  0.802149  0.880729  2.073757e-06  0.999996  2.357311e-07 -3.997955e-06   \n",
       "1996  0.227937  0.215761  4.018471e-07 -0.000009 -1.054975e-05  9.999933e-01   \n",
       "1997  1.146383  0.183903  2.062651e-05 -0.000007  1.000009e+00  8.974153e-06   \n",
       "1998  0.590797  0.479184  1.000018e+00 -0.000008  6.802510e-06 -1.569116e-06   \n",
       "1999  0.785764  1.719588  1.000007e+00  0.000013 -1.485859e-05  6.585560e-06   \n",
       "\n",
       "      x4_MZBER   x4_PQKE       x4_YEQA   x4_ZUQF  ...          x7_5  \\\n",
       "0     1.000000 -0.000005 -1.462577e-06  0.000013  ... -9.194131e-06   \n",
       "1     1.000001 -0.000003  2.094801e-06 -0.000002  ... -8.121052e-06   \n",
       "2     1.000024  0.000003  8.410662e-06 -0.000008  ... -5.184427e-09   \n",
       "3     1.000000  0.000008  4.903438e-07 -0.000018  ...  1.581487e-05   \n",
       "4     0.999980 -0.000007 -2.803982e-06  0.000024  ...  9.999890e-01   \n",
       "...        ...       ...           ...       ...  ...           ...   \n",
       "1995  1.000014 -0.000004  3.130863e-06 -0.000012  ... -5.535543e-08   \n",
       "1996  0.000001  0.000002  9.999761e-01  0.000007  ...  6.330760e-06   \n",
       "1997  1.000004 -0.000004  1.008618e-05 -0.000003  ... -2.970408e-06   \n",
       "1998  1.000010 -0.000011 -1.523550e-05 -0.000010  ... -2.392276e-06   \n",
       "1999  0.999993  0.000010 -9.329268e-06  0.000018  ...  1.192127e-05   \n",
       "\n",
       "              x7_6          x7_8   x8_-7.5   x8_-6.5   x8_-5.5   x8_-4.5  \\\n",
       "0     1.000006e+00 -5.642623e-06  0.000019  0.999997 -0.000017  0.000022   \n",
       "1     9.999939e-01  8.609777e-06 -0.000003  0.000003 -0.000011  1.000010   \n",
       "2     1.879509e-05 -1.394652e-05  0.000012  0.000002  0.000004 -0.000002   \n",
       "3    -1.290875e-06  9.696633e-07 -0.000014  0.000009  0.000008  1.000006   \n",
       "4     9.591986e-06 -3.521323e-06 -0.000005  0.000006 -0.000006  1.000005   \n",
       "...            ...           ...       ...       ...       ...       ...   \n",
       "1995  6.291697e-06 -1.015886e-05  0.000005  0.000003 -0.000013 -0.000011   \n",
       "1996  1.188480e-05 -1.891222e-05 -0.000009 -0.000011  0.000008 -0.000021   \n",
       "1997  6.588313e-06  8.169397e-06 -0.000006 -0.000002  0.999986  0.000025   \n",
       "1998 -3.480053e-06 -1.184995e-05 -0.000018 -0.000010  0.999999 -0.000009   \n",
       "1999  6.428366e-07  8.511502e-06 -0.000027 -0.000001  0.999990 -0.000016   \n",
       "\n",
       "       x8_-3.5   x8_-2.5       x8_-1.5  \n",
       "0    -0.000002  0.000004 -1.600673e-05  \n",
       "1    -0.000007 -0.000006  1.185335e-05  \n",
       "2     0.000023  0.999996  6.143002e-06  \n",
       "3    -0.000009 -0.000010 -2.666942e-06  \n",
       "4    -0.000002 -0.000011 -1.149327e-05  \n",
       "...        ...       ...           ...  \n",
       "1995 -0.000007  1.000000  2.781661e-06  \n",
       "1996  0.000014  1.000009 -9.735054e-06  \n",
       "1997 -0.000011 -0.000018 -8.635924e-07  \n",
       "1998 -0.000001 -0.000007 -2.809606e-06  \n",
       "1999  0.000004  0.000002 -1.323412e-05  \n",
       "\n",
       "[2000 rows x 26 columns]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# add noise to cathegorial features\n",
    "p = 1e-5\n",
    "\n",
    "# add noise for every feature separately\n",
    "X = X + np.random.normal(0,p,X.shape)\n",
    "X\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform data into a tensor\n",
    "x = torch.tensor(X.values,dtype=torch.float32)\n",
    "y = torch.tensor(Y.values,dtype=torch.float32).reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(torch.nn.Module):\n",
    "    def __init__(self, n_feature, n_hidden, n_output):\n",
    "        super(Net, self).__init__()\n",
    "        # hidden layer\n",
    "        self.hidden = torch.nn.Linear(n_feature, n_hidden)\n",
    "        #output layer\n",
    "        self.out = torch.nn.Linear(n_hidden, n_output)\n",
    "        self.sigmoid = nn.Sigmoid() # Output between 0 and 1\n",
    "    def forward(self, x):\n",
    "        #activation function for the hidden layer\n",
    "        x = F.sigmoid(self.hidden(x))\n",
    "        x = self.out(x)  \n",
    "        x = self.sigmoid(x)\n",
    "        # linear output\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.6150, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "tensor(0.6148, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "tensor(0.6146, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "tensor(0.6144, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "tensor(0.6142, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "tensor(0.6140, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "tensor(0.6138, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "tensor(0.6137, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "tensor(0.6135, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "tensor(0.6133, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Confusion Matrix\n",
      "[[1978    0]\n",
      " [  22    0]]\n"
     ]
    }
   ],
   "source": [
    "net = Net(n_feature=26,n_hidden=32,n_output=1)\n",
    "optimizer = torch.optim.SGD(net.parameters(),lr=1e-5)\n",
    "weights = torch.tensor([100, 1])  # Example weights for class 0 and 1\n",
    "loss_func = torch.nn.BCELoss()\n",
    "\n",
    "\n",
    "for t in range(100):    \n",
    "    out = net(x)\n",
    "    # input x and predict based on x\n",
    "\n",
    "    loss = loss_func(out, y)     \n",
    "    # must be (1. nn output, 2. target)\n",
    "    optimizer.zero_grad()   # clear gradients for next train\n",
    "    loss.backward()         \n",
    "    # backpropagation, compute gradients\n",
    "    optimizer.step()        \n",
    "    # apply gradients\n",
    "    if t % 10 == 0:\n",
    "        # print the loss every 10 iterations\n",
    "        print(loss)\n",
    "\n",
    "# Count how much elements doesn't match for every class, 1 or 0\n",
    "\n",
    "# print the confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "print('Confusion Matrix')\n",
    "print(confusion_matrix(y.data.numpy(), predicted.data.numpy()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1978"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(sum(correct == True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Deep(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layer1 = nn.Linear(26, 100)\n",
    "        self.act1 = nn.ReLU()\n",
    "        self.layer2 = nn.Linear(100, 100)\n",
    "        self.act2 = nn.ReLU()\n",
    "        self.layer3 = nn.Linear(100, 100)\n",
    "        self.act3 = nn.ReLU()\n",
    "        self.output = nn.Linear(100, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.act1(self.layer1(x))\n",
    "        x = self.act2(self.layer2(x))\n",
    "        x = self.act3(self.layer3(x))\n",
    "        x = self.sigmoid(self.output(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import tqdm\n",
    "# import f1 score\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def model_train(model, X_train, y_train, X_val, y_val):\n",
    "    # loss function and optimizer\n",
    "    # weights = torch.tensor([100, 1])  # Example weights for class 0 and 1\n",
    "    # loss_func = torch.nn.BCELoss(weights.type(torch.FloatTensor))  \n",
    "    weights = torch.tensor([100, 1]) \n",
    "    loss_fn = nn.BCELoss()  # binary cross entropy\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "\n",
    "    n_epochs = 10   # number of epochs to run\n",
    "    batch_size = 10  # size of each batch\n",
    "    batch_start = torch.arange(0, len(X_train), batch_size)\n",
    "\n",
    "    # Hold the best model\n",
    "    best_acc = - np.inf   # init to negative infinity\n",
    "    best_weights = None\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        print(\"Epoch: \",epoch)\n",
    "        # print(epoch)\n",
    "        model.train()\n",
    "        with tqdm.tqdm(batch_start, unit=\"batch\", mininterval=0, disable=True) as bar:\n",
    "            bar.set_description(f\"Epoch {epoch}\")\n",
    "            for start in bar:\n",
    "                # take a batch\n",
    "                X_batch = X_train[start:start+batch_size]\n",
    "                y_batch = y_train[start:start+batch_size]\n",
    "                # forward pass\n",
    "                y_pred = model(X_batch)\n",
    "                loss = loss_fn(y_pred, y_batch)\n",
    "                # backward pass\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                # update weights\n",
    "                optimizer.step()\n",
    "                # print progress\n",
    "                acc = (y_pred.round() == y_batch).float().mean()\n",
    "                bar.set_postfix(\n",
    "                    loss=float(loss),\n",
    "                    acc=float(acc)\n",
    "                )\n",
    "        # evaluate accuracy at end of each epoch\n",
    "        model.eval()\n",
    "        y_pred = model(X_val)\n",
    "        acc = (y_pred.round() == y_val).float().mean()\n",
    "        acc = float(acc)\n",
    "        #compute f1 score\n",
    "        f1 = f1_score(y_val.detach().numpy(), y_pred.round().detach().numpy())\n",
    "        # print f1 score\n",
    "        print(f'F1 score: {f1}')\n",
    "        if acc > best_acc:\n",
    "            best_acc = acc\n",
    "            best_weights = copy.deepcopy(model.state_dict())\n",
    "    # restore model and return best accuracy\n",
    "    model.load_state_dict(best_weights)\n",
    "    return best_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0\n",
      "F1 score: 0.0\n",
      "Epoch:  1\n",
      "F1 score: 0.0\n",
      "Epoch:  2\n",
      "F1 score: 0.0\n",
      "Epoch:  3\n",
      "F1 score: 0.0\n",
      "Epoch:  4\n",
      "F1 score: 0.0\n",
      "Epoch:  5\n",
      "F1 score: 0.0\n",
      "Epoch:  6\n",
      "F1 score: 0.0\n",
      "Epoch:  7\n",
      "F1 score: 0.0\n",
      "Epoch:  8\n",
      "F1 score: 0.0\n",
      "Epoch:  9\n",
      "F1 score: 0.0\n",
      "Accuracy (wide): 0.99\n",
      "Epoch:  0\n",
      "F1 score: 0.0\n",
      "Epoch:  1\n",
      "F1 score: 0.0\n",
      "Epoch:  2\n",
      "F1 score: 0.0\n",
      "Epoch:  3\n",
      "F1 score: 0.0\n",
      "Epoch:  4\n",
      "F1 score: 0.0\n",
      "Epoch:  5\n",
      "F1 score: 0.0\n",
      "Epoch:  6\n",
      "F1 score: 0.0\n",
      "Epoch:  7\n",
      "F1 score: 0.0\n",
      "Epoch:  8\n",
      "F1 score: 0.0\n",
      "Epoch:  9\n",
      "F1 score: 0.0\n",
      "Accuracy (wide): 0.99\n",
      "Epoch:  0\n",
      "F1 score: 0.0\n",
      "Epoch:  1\n",
      "F1 score: 0.0\n",
      "Epoch:  2\n",
      "F1 score: 0.0\n",
      "Epoch:  3\n",
      "F1 score: 0.0\n",
      "Epoch:  4\n",
      "F1 score: 0.0\n",
      "Epoch:  5\n",
      "F1 score: 0.0\n",
      "Epoch:  6\n",
      "F1 score: 0.0\n",
      "Epoch:  7\n",
      "F1 score: 0.0\n",
      "Epoch:  8\n",
      "F1 score: 0.0\n",
      "Epoch:  9\n",
      "F1 score: 0.0\n",
      "Accuracy (wide): 0.99\n",
      "Epoch:  0\n",
      "F1 score: 0.0\n",
      "Epoch:  1\n",
      "F1 score: 0.0\n",
      "Epoch:  2\n",
      "F1 score: 0.0\n",
      "Epoch:  3\n",
      "F1 score: 0.0\n",
      "Epoch:  4\n",
      "F1 score: 0.0\n",
      "Epoch:  5\n",
      "F1 score: 0.0\n",
      "Epoch:  6\n",
      "F1 score: 0.0\n",
      "Epoch:  7\n",
      "F1 score: 0.0\n",
      "Epoch:  8\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[48], line 10\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m train, test \u001b[38;5;129;01min\u001b[39;00m kfold\u001b[38;5;241m.\u001b[39msplit(x, y):\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;66;03m# create model, train, and get accuracy\u001b[39;00m\n\u001b[0;32m      9\u001b[0m     model \u001b[38;5;241m=\u001b[39m Deep()\n\u001b[1;32m---> 10\u001b[0m     acc \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtest\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtest\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAccuracy (wide): \u001b[39m\u001b[38;5;132;01m%.2f\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m acc)\n\u001b[0;32m     13\u001b[0m     cv_scores\u001b[38;5;241m.\u001b[39mappend(acc)\n",
      "Cell \u001b[1;32mIn[47], line 42\u001b[0m, in \u001b[0;36mmodel_train\u001b[1;34m(model, X_train, y_train, X_val, y_val)\u001b[0m\n\u001b[0;32m     40\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m     41\u001b[0m \u001b[38;5;66;03m# update weights\u001b[39;00m\n\u001b[1;32m---> 42\u001b[0m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;66;03m# print progress\u001b[39;00m\n\u001b[0;32m     44\u001b[0m acc \u001b[38;5;241m=\u001b[39m (y_pred\u001b[38;5;241m.\u001b[39mround() \u001b[38;5;241m==\u001b[39m y_batch)\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39mmean()\n",
      "File \u001b[1;32mc:\\Users\\aaihs\\anaconda3\\envs\\pt\\lib\\site-packages\\torch\\optim\\optimizer.py:487\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    482\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    483\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    484\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    485\u001b[0m             )\n\u001b[1;32m--> 487\u001b[0m out \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    488\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[0;32m    490\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\aaihs\\anaconda3\\envs\\pt\\lib\\site-packages\\torch\\optim\\optimizer.py:91\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     89\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m     90\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[1;32m---> 91\u001b[0m     ret \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     92\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     93\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
      "File \u001b[1;32mc:\\Users\\aaihs\\anaconda3\\envs\\pt\\lib\\site-packages\\torch\\optim\\adam.py:223\u001b[0m, in \u001b[0;36mAdam.step\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    211\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    213\u001b[0m     has_complex \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_group(\n\u001b[0;32m    214\u001b[0m         group,\n\u001b[0;32m    215\u001b[0m         params_with_grad,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    220\u001b[0m         state_steps,\n\u001b[0;32m    221\u001b[0m     )\n\u001b[1;32m--> 223\u001b[0m     \u001b[43madam\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    224\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    225\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    226\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    227\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    228\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    229\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    230\u001b[0m \u001b[43m        \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mamsgrad\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    231\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    232\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    233\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    234\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    235\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    236\u001b[0m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    237\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    238\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforeach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mforeach\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    239\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcapturable\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    240\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdifferentiable\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    241\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfused\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfused\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    242\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgrad_scale\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    243\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfound_inf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    244\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    246\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[1;32mc:\\Users\\aaihs\\anaconda3\\envs\\pt\\lib\\site-packages\\torch\\optim\\optimizer.py:154\u001b[0m, in \u001b[0;36m_disable_dynamo_if_unsupported.<locals>.wrapper.<locals>.maybe_fallback\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m disabled_func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    153\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 154\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\aaihs\\anaconda3\\envs\\pt\\lib\\site-packages\\torch\\optim\\adam.py:784\u001b[0m, in \u001b[0;36madam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[0;32m    781\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    782\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adam\n\u001b[1;32m--> 784\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    785\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    786\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    787\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    788\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    789\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    790\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    791\u001b[0m \u001b[43m    \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    792\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    793\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    794\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    795\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    796\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    797\u001b[0m \u001b[43m    \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    798\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    799\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    800\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    801\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    802\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfound_inf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    803\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\aaihs\\anaconda3\\envs\\pt\\lib\\site-packages\\torch\\optim\\adam.py:378\u001b[0m, in \u001b[0;36m_single_tensor_adam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, has_complex, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[0;32m    375\u001b[0m     param \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mview_as_real(param)\n\u001b[0;32m    377\u001b[0m \u001b[38;5;66;03m# Decay the first and second moment running average coefficient\u001b[39;00m\n\u001b[1;32m--> 378\u001b[0m \u001b[43mexp_avg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlerp_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    379\u001b[0m exp_avg_sq\u001b[38;5;241m.\u001b[39mmul_(beta2)\u001b[38;5;241m.\u001b[39maddcmul_(grad, grad\u001b[38;5;241m.\u001b[39mconj(), value\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m beta2)\n\u001b[0;32m    381\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m capturable \u001b[38;5;129;01mor\u001b[39;00m differentiable:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "\n",
    "# train-test split: Hold out the test set for final model evaluation\n",
    "# define 5-fold cross validation test harness\n",
    "kfold = StratifiedKFold(n_splits=5, shuffle=True)\n",
    "cv_scores = []\n",
    "for train, test in kfold.split(x, y):\n",
    "    # create model, train, and get accuracy\n",
    "    model = Deep()\n",
    "    acc = model_train(model, x[train], y[train], x[test], y[test])\n",
    "    print(\"Accuracy (wide): %.2f\" % acc)\n",
    "\n",
    "    cv_scores.append(acc)\n",
    "\n",
    "# evaluate the model\n",
    "acc = np.mean(cv_scores)\n",
    "std = np.std(cv_scores)\n",
    "print(\"Model accuracy: %.2f%% (+/- %.2f%%)\" % (acc*100, std*100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
